{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import cohere\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from config import Paths, CohereConfig \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tiktoken import get_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(elem: BeautifulSoup):\n",
    "    text = ''\n",
    "    for e in elem.descendants:\n",
    "        if isinstance(e, str):\n",
    "            text += e.strip()\n",
    "        elif e.name in ['br', 'p', 'h1', 'h2', 'h3', 'h4','tr', 'th']:\n",
    "            text += '\\n'\n",
    "        elif e.name == 'li':\n",
    "            text += '\\n- '\n",
    "    return text\n",
    "\n",
    "def get_sections_by_tag(soup: BeautifulSoup, tag: str) -> dict:\n",
    "    tags = soup.find_all(tag)\n",
    "\n",
    "    if len(tags) == 0:\n",
    "        raise ValueError(f\"No {tag} tags found in the document\")\n",
    "\n",
    "    output = []\n",
    "\n",
    "    elements = []\n",
    "    for elem in soup.find_all():\n",
    "        if elem != tags[0]:\n",
    "            elements.append(str(elem))\n",
    "        else:\n",
    "            break\n",
    "    output.append(\n",
    "        {\"section_name\": \"\", \"section_elements\": elements}\n",
    "    )\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        # Get the current tag\n",
    "        current_tag = tags[i]\n",
    "\n",
    "        # Get the next tag, if it exists\n",
    "        if i < len(tags) - 1:\n",
    "            next_tag = tags[i + 1]\n",
    "        else:\n",
    "            next_tag = None\n",
    "\n",
    "        # Find all elements between the current tag and the next tag\n",
    "        elements = []\n",
    "        next_sibling = current_tag.next_sibling\n",
    "        while next_sibling and next_sibling != next_tag:\n",
    "            if next_sibling.name != tag:\n",
    "                elements.append(str(next_sibling))\n",
    "            next_sibling = next_sibling.next_sibling\n",
    "\n",
    "        output.append(\n",
    "            {\"section_name\": parse_html(current_tag), \"section_elements\": elements}\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "def split_by_element(soup: BeautifulSoup, parent_name: str) -> dict:\n",
    "    output = []\n",
    "    curr_text, curr_elements = \"\", []\n",
    "    for element in soup.find_all():\n",
    "        text = parse_html(element)\n",
    "        if len(tokenizer.encode(curr_text + text)) < 512:\n",
    "            curr_text += \" \" + text\n",
    "            curr_elements.append(str(element))\n",
    "        else:\n",
    "            text_formatted = parse_html(BeautifulSoup(\"\\n\".join(curr_elements), \"html.parser\"))\n",
    "            if text_formatted:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"section\": f'{parent_name}',\n",
    "                        \"text\": text_formatted,\n",
    "                        \"elements\": curr_elements,\n",
    "                    }\n",
    "                )\n",
    "            curr_text, curr_elements = \"\", []\n",
    "    return output\n",
    "\n",
    "def split_by_hierarchy(soup: BeautifulSoup, parent_name: str) -> dict:\n",
    "    output = []\n",
    "    try:\n",
    "        h3s = get_sections_by_tag(soup, \"h3\")\n",
    "        for h3 in h3s:\n",
    "            soup = BeautifulSoup(\n",
    "                \"\\n\".join(h3[\"section_elements\"]), \"html.parser\"\n",
    "            )\n",
    "            text = parse_html(soup)\n",
    "            if len(tokenizer.encode(text)) < 512:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"section\": f'{parent_name}>{h3[\"section_name\"]}',\n",
    "                        \"text\": h3[\"section_name\"] + \"\\n\" + text,\n",
    "                        \"elements\": h3[\"section_elements\"],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    h4s = get_sections_by_tag(soup, \"h4\")\n",
    "                    for h4 in h4s:\n",
    "                        soup = BeautifulSoup(\n",
    "                            \"\\n\".join(h4[\"section_elements\"]), \"html.parser\"\n",
    "                        )\n",
    "                        text = parse_html(soup)\n",
    "                        if len(tokenizer.encode(text)) < 512:\n",
    "                            output.append(\n",
    "                                {\n",
    "                                    \"section\": f'{parent_name}>{h3[\"section_name\"]}>{h4[\"section_name\"]}',\n",
    "                                    \"text\": h4[\"section_name\"] + \"\\n\" + text,\n",
    "                                    \"elements\": h4[\"section_elements\"],\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            output.extend(\n",
    "                                split_by_element(soup, f'{parent_name}>{h3[\"section_name\"]}>{h4[\"section_name\"]}')\n",
    "                            )\n",
    "                except ValueError:\n",
    "                    output.extend(split_by_element(soup, f'{parent_name}>{h3[\"section_name\"]}'))\n",
    "    except ValueError:\n",
    "        output.extend(split_by_element(soup, parent_name))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"handbook_total-rewards_benefits_parental-leave-toolkit-index.json\"\n",
    "\n",
    "handbook_parsed = Path(f\"{Paths.data}/handbook_parsed\")\n",
    "handbook_processed = Path(f\"{Paths.data}/handbook_processed\")\n",
    "\n",
    "for f in handbook_processed.glob(\"*.json\"):\n",
    "    f.unlink()\n",
    "\n",
    "for f in handbook_parsed.glob(\"*.json\"):\n",
    "    output = []\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for section in data[\"elements\"]:\n",
    "            soup = BeautifulSoup(\"\\n\".join(section[\"section_elements\"]), \"html.parser\")\n",
    "            text = parse_html(soup)\n",
    "            if len(tokenizer.encode(text)) < 512:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"title\": data[\"title\"],\n",
    "                        \"path\": data[\"path\"],\n",
    "                        \"section\": section[\"section_name\"],\n",
    "                        \"text\": text,\n",
    "                        \"elements\": section[\"section_elements\"],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                reduced_elements = split_by_hierarchy(soup, section[\"section_name\"])\n",
    "                for element in reduced_elements:\n",
    "                    output.append(\n",
    "                        {\n",
    "                            \"title\": data[\"title\"],\n",
    "                            \"path\": data[\"path\"],\n",
    "                            \"section\": element[\"section\"],\n",
    "                            \"text\": element[\"text\"],\n",
    "                            \"elements\": element[\"elements\"],\n",
    "                        }\n",
    "                    )\n",
    "    filename = f.name.replace(f\"{Paths.data}/handbook_parsed\", \"\")\n",
    "    full_filename = f\"{Paths.data}/handbook_processed{filename}\"    \n",
    "    with open(full_filename, \"w\") as fw:\n",
    "        json.dump(output, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "handbook_merged = Path(f\"{Paths.data}/handbook_merged\")\n",
    "full_filename = f\"{handbook_merged}/handbook.json\"\n",
    "\n",
    "output_merged = []\n",
    "for f in handbook_processed.glob(\"*.json\"):\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        output_merged.extend(data)\n",
    "\n",
    "with open(full_filename, \"w\") as fw:\n",
    "    json.dump(output_merged, fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7dcbf1e03c02e24dc4a584a70e4e23a95f5bc1447200fb3184f76f97c0e1aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
