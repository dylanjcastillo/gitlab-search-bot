{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dcast/Documents/GitHub/gitlab-search-bot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import cohere\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from config import Paths, APIKeys \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections_by_tag(soup: BeautifulSoup, tag: str) -> dict:\n",
    "    tags = soup.find_all(tag)\n",
    "\n",
    "    if len(tags) == 0:\n",
    "        raise ValueError(f\"No {tag} tags found in the document\")\n",
    "\n",
    "    output = []\n",
    "\n",
    "    elements = []\n",
    "    for elem in soup.find_all():\n",
    "        if elem != tags[0]:\n",
    "            elements.append(str(elem))\n",
    "        else:\n",
    "            break\n",
    "    output.append(\n",
    "        {\"section_name\": \"\", \"section_elements\": elements}\n",
    "    )\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        # Get the current tag\n",
    "        current_tag = tags[i]\n",
    "\n",
    "        # Get the next tag, if it exists\n",
    "        if i < len(tags) - 1:\n",
    "            next_tag = tags[i + 1]\n",
    "        else:\n",
    "            next_tag = None\n",
    "\n",
    "        # Find all elements between the current tag and the next tag\n",
    "        elements = []\n",
    "        next_sibling = current_tag.next_sibling\n",
    "        while next_sibling and next_sibling != next_tag:\n",
    "            if next_sibling.name != tag:\n",
    "                elements.append(str(next_sibling))\n",
    "            next_sibling = next_sibling.next_sibling\n",
    "\n",
    "        output.append(\n",
    "            {\"section_name\": current_tag.get_text(), \"section_elements\": elements}\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "def split_by_element(soup: BeautifulSoup, parent_name: str) -> dict:\n",
    "    output = []\n",
    "    curr_text, curr_elements = \"\", []\n",
    "    for element in soup.find_all():\n",
    "        text = element.get_text()\n",
    "        if len(tokenizer(curr_text + text, padding=False, truncation=False)[\"input_ids\"]) < 512:\n",
    "            curr_text += \" \" + text\n",
    "            curr_elements.append(str(element))\n",
    "        else:\n",
    "            text_formatted = BeautifulSoup(\"\\n\".join(curr_elements), \"html.parser\").get_text().strip()\n",
    "            if text_formatted:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"section\": f'{parent_name}',\n",
    "                        \"text\": text_formatted,\n",
    "                        \"elements\": curr_elements,\n",
    "                    }\n",
    "                )\n",
    "            curr_text, curr_elements = \"\", []\n",
    "    return output\n",
    "\n",
    "def split_by_hierarchy(soup: BeautifulSoup, parent_name: str) -> dict:\n",
    "    output = []\n",
    "    try:\n",
    "        h3s = get_sections_by_tag(soup, \"h3\")\n",
    "        for h3 in h3s:\n",
    "            soup = BeautifulSoup(\n",
    "                \"\\n\".join(h3[\"section_elements\"]), \"html.parser\"\n",
    "            )\n",
    "            text = soup.get_text().strip()\n",
    "            if len(tokenizer(text, padding=False, truncation=False)[\"input_ids\"]) < 512:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"section\": f'{parent_name}>{h3[\"section_name\"]}',\n",
    "                        \"text\": h3[\"section_name\"] + \"\\n\" + text,\n",
    "                        \"elements\": h3[\"section_elements\"],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    h4s = get_sections_by_tag(soup, \"h4\")\n",
    "                    for h4 in h4s:\n",
    "                        soup = BeautifulSoup(\n",
    "                            \"\\n\".join(h4[\"section_elements\"]), \"html.parser\"\n",
    "                        )\n",
    "                        text = soup.get_text().strip()\n",
    "                        if len(tokenizer(text, padding=False, truncation=False)[\"input_ids\"]) < 512:\n",
    "                            output.append(\n",
    "                                {\n",
    "                                    \"section\": f'{parent_name}>{h3[\"section_name\"]}>{h4[\"section_name\"]}',\n",
    "                                    \"text\": h4[\"section_name\"] + \"\\n\" + text,\n",
    "                                    \"elements\": h4[\"section_elements\"],\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            output.extend(\n",
    "                                split_by_element(soup, f'{parent_name}>{h3[\"section_name\"]}>{h4[\"section_name\"]}')\n",
    "                            )\n",
    "                except ValueError:\n",
    "                    output.extend(split_by_element(soup, f'{parent_name}>{h3[\"section_name\"]}'))\n",
    "    except ValueError:\n",
    "        output.extend(split_by_element(soup, parent_name))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# test_file = \"handbook_total-rewards_benefits_parental-leave-toolkit-index.json\"\n",
    "\n",
    "handbook_parsed = Path(f\"{Paths.data}/handbook_parsed\")\n",
    "handbook_processed = Path(f\"{Paths.data}/handbook_processed\")\n",
    "\n",
    "for f in handbook_processed.glob(\"*.json\"):\n",
    "    f.unlink()\n",
    "\n",
    "for f in handbook_parsed.glob(\"*.json\"):\n",
    "    output = []\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        for section in data[\"elements\"]:\n",
    "            soup = BeautifulSoup(\"\\n\".join(section[\"section_elements\"]), \"html.parser\")\n",
    "            text = soup.get_text()\n",
    "            if len(tokenizer(text, padding=False, truncation=False)[\"input_ids\"]) < 512:\n",
    "                output.append(\n",
    "                    {\n",
    "                        \"title\": data[\"title\"],\n",
    "                        \"path\": data[\"path\"],\n",
    "                        \"section\": section[\"section_name\"],\n",
    "                        \"text\": text,\n",
    "                        \"elements\": section[\"section_elements\"],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                reduced_elements = split_by_hierarchy(soup, section[\"section_name\"])\n",
    "                for element in reduced_elements:\n",
    "                    output.append(\n",
    "                        {\n",
    "                            \"title\": data[\"title\"],\n",
    "                            \"path\": data[\"path\"],\n",
    "                            \"section\": element[\"section\"],\n",
    "                            \"text\": element[\"text\"],\n",
    "                            \"elements\": element[\"elements\"],\n",
    "                        }\n",
    "                    )\n",
    "    filename = f.name.replace(f\"{Paths.data}/handbook_parsed\", \"\")\n",
    "    full_filename = f\"{Paths.data}/handbook_processed{filename}\"    \n",
    "    with open(full_filename, \"w\") as fw:\n",
    "        json.dump(output, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handbook_merged = Path(f\"{Paths.data}/handbook_merged\")\n",
    "full_filename = f\"{handbook_merged}/handbook.json\"\n",
    "\n",
    "output_merged = []\n",
    "for f in handbook_processed.glob(\"*.json\"):\n",
    "    with open(f, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        output_merged.extend(data)\n",
    "\n",
    "with open(full_filename, \"w\") as fw:\n",
    "    json.dump(output_merged, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "co = cohere.Client(APIKeys.cohere)\n",
    "\n",
    "@cache\n",
    "def get_embeddings(batch: tuple):\n",
    "    response = co.embed(\n",
    "        model='large',\n",
    "        texts=list(batch)\n",
    "    )\n",
    "    return response.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26488\n",
      "Encoding batch 96\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "batch_size = 96 \n",
    "batch = []\n",
    "\n",
    "for i, elem in enumerate(output_merged):\n",
    "    if i % 500 == 0:\n",
    "        print(i, len(output_merged))\n",
    "\n",
    "    if len(elem[\"text\"]) > 5:\n",
    "        batch.append(\"\\n\".join(elem[\"text\"]))\n",
    "\n",
    "    if len(batch) >= batch_size:\n",
    "        print(\"Encoding batch\", len(batch))\n",
    "        vectors.append(get_embeddings(tuple(batch)))\n",
    "        batch = []\n",
    "        break\n",
    "\n",
    "if len(batch) > 0:\n",
    "    vectors.append(get_embeddings(tuple(batch)))\n",
    "    batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, v in zip(output_merged, vectors[0]):\n",
    "    m[\"vector\"] = v.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4446302883636793\n",
      "Synthetic data production is the creation of 3d models with textures that are used in out ML training. These models are typically built and textured in Blender.\n",
      "Time Estimation\n",
      "This  spread sheet  can be used to calculate the time to build the product.\n",
      "Conclusions\n",
      "The six images provide a relatively quick solution to creating synthetic replicas.\n",
      "With the six image process it is possible to create products within days of meeting with the customer. There is no question that this has to be one of the most time efficient processes. For a large proportion of boxed goods this process is efficient and effective.\n",
      "---\n",
      "Score: 0.44143337105584224\n",
      "The six image process does not provide good labels on products with curved and complex surfaces and it would be better to extract the labels and scan them on a flat bed scanner. Scanner labels are better quality (in colour and distortion) and are quicker to place on the model.\n",
      "More complex models and more accurate surface representation are better achieved by the artist having direct access to the product packaging.\n",
      "The time to produce a digital replica has a few factors:\n",
      "Complexity of the surface - the less like a box or cylinder the longer it will take.\n",
      "---\n",
      "Score: 0.38486315650151115\n",
      "apparently state of art is hard to do well on their dataset. is it because it’s so good, or the domain gap? - they used Dense Prediction Transformer\n",
      "They use Blender and cycles.\n",
      "3D modelling\n",
      "Each store model is a to-scale replica of a real retail store. To create these\n",
      "replica assets, we perform a 3d scan of the store using a lidar-based Matterport\n",
      "[2] device, and utilize an in-house asset creator to model the store in Blender with\n",
      "the 3d scan as a guide.\n",
      "---\n",
      "Score: 0.36967777869792784\n",
      "CV\n",
      "The only part of the recycling chain that uses vision is currently being automated by robots. As the conveyor belts are moving so quickly, a human would not have time to look at an object detection system and then look back to remove something. Therefore, ***REMOVED*** will need to partner with the robotics companies to improve the computer vision for their robots.\n",
      "Synthetic data\n",
      "Question to be answered: what kind of data do we see on the conveyor belts? How much does a single object type vary in its model? Can software automatically model deformed objects?\n",
      "Companies of interest\n",
      "AMP Robotics\n",
      "Zenrobotics\n",
      "---\n",
      "Score: 0.36913544610446636\n",
      "High-fidelity for 3D assets is really important (fidelity=how similar they are to real objects)\n",
      "Accurately representing scale as in the real images is  important\n",
      "When testing a DR approach versus 3D Scene for a real scene dataset, the results are   similar  (super-robotics, always-mkt).\n",
      "Mimicking the real data properties (scale, rotation, hsv) gets you good results for 3D scenes.\n",
      "Mimicking the context in the real data is important.\n",
      "---\n",
      "Score: 0.36605741130478525\n",
      "Robotics can be used as part the manufacturing process. Robots that rely on vision could have their model accuracy increased through synthetic data.\n",
      "Companies of interest\n",
      "Landling AI  (Andrew Ng)\n",
      "UVEye  (company using vision underneath a car)\n",
      "Mobidev  (company that provides visual inspection consulting)\n",
      "Osaro  (company that builds robots for industrial applications - claims advanced Vision system)\n",
      "MicroPSI  (industrial robots)\n",
      "Background reading\n",
      "10 examples of machine learning in manufacturing\n",
      "Case studies\n",
      "Nuclear waste\n",
      "Domain summary\n",
      "The nuclear industry generates a large amount of radioactive waste which requires proper and safe disposal. The categories of waste are:\n",
      "---\n",
      "Score: 0.3577460342781428\n",
      "Start with the scale that is  closest  to the camera and gradually move away. The foremost objects appear larger and thus are  easier for the network to learn  and as they move away, they become smaller and more difficult to learn\n",
      "For each scale they iterate all  possible plane rotations  (which probably ensures  realistic 3D rendering ???)\n",
      "---\n",
      "Score: 0.35097303376315925\n",
      "Go over the general context of why synthetic data came to be. Go quickly over the progress of computer vision and neural networks, but mention the fact that they are data hungry. Mention a few alternatives to leverage this: amazon mechanical turk, captcha, etc. And then mention synthetic data generation.\n",
      "---\n",
      "Score: 0.3335868373240395\n",
      "Simpler problems like road segmentation or road sign detection are easily solved with synthetic data\n",
      "The more complex the scenario, indoor/outdoor complex scenarios, synthetic data hasn't performed so far.\n",
      "Recent advances in rendering technology (UI4?) - unreal? photorealistic capture!\n",
      "Some stuff they do.\n",
      "Build simulator and varied scenes of freeway examples:\n",
      "They do procedural generation and focus on asset diversity!\n",
      "Neural rendering, they are investing in this —> will solve asset diversity. Hard problem, need a few more $$$ to hire people to work on that.\n",
      "---\n",
      "Score: 0.3220174248148307\n",
      "Authors train using  Full Domain Randomization  with 3D objects, placed in front of realistic backgrounds, and demonstrate that their  simple  approach trains detectors that  outperform  models trained on real data (!!!).\n",
      "Discussion on how to bridge the  Gap\n",
      "High Randomization yields better results because  reality is often a subset  of the domain (!!!)\n",
      "They use 3D backgrounds which are  Rendered  in a randomized fashion to create  realistic  backgrounds\n",
      "All foreground models are presented to the network equally under all possible poses with increasing complexity.\n",
      "They don't use complex scene composition, photo-realism or real background images\n",
      "---\n",
      "Score: 0.32169127286238497\n",
      "The current six image process make it difficult for the artist to recreate small surface details. Many branded bottles have contours and grip patterns that cannot be measured from the six images. Capturing these small details accurately is very difficult without high quality scanning or access to digital sources.\n",
      "Physical product vs Images\n",
      "---\n",
      "Score: 0.32074207790851195\n",
      "The generated images showcases in the paper actually look extremely realistic and plausible\n",
      "They do not do a  random  pose, but use a curriculum ( incremental   strategy or algorithm ) which embeds some kind of logic and geometry - which in turn produces realistic 3D scenes\n",
      "The properties of the background  significantly  improve performance of the classifier\n",
      "Relative smaller scale helps because the network learns to distinguish objects in the background\n",
      "A higher number of rendered foreground objects yields better performance (8-9 objects being the max)\n",
      "---\n",
      "Score: 0.31562185852440217\n",
      "Nevertheless, and regardless of its potential, synthetic data has significant challenges of its own. Firstly, in order for it to train a model that performs well on a real setting, the scene from which synthetic data is generated has to be carefully set and developed, an expense of time that can contradict the initial goal of synthetic data, which is speeding up this process. Secondly, a significant amount of time is required to generate a variety of scenes that resemble the randomness of the real data, and thus allow for the training of more robust object detection/classification systems. In an attempt to solve the aforementioned challenges, \\cite{Tremblay2018TrainingRandomization} proposes using Domain Randomization to randomize the given scene in the hopes of increasing the robustness of the trained model. This is similarly done by \\cite{Bousmalis2016DomainNetworks} where they use Domain Separation Networks to train models with synthetic data that renders the model domain invariant, and therefore more robust during test time.\n",
      "---\n",
      "Score: 0.30842816146287444\n",
      "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation  2020 ( pdf )\n",
      "While 3d models are easy to come by, generating scenes requires humans looking at lots of images and generating procedural models, such as Probabilistic Scene Grammars (Meta-Sim). This paper uses RL to generate a procedural model instead.\n",
      "Data augmentation\n",
      "Modeling Visual Context is Key to Augmenting Object Detection Datasets  2018 ( pdf )\n",
      "---\n",
      "Score: 0.30217527225301516\n",
      "A success metric for the students involved would be to publish a conference paper or workshop paper at a leading conference in computer vision such as ICCV, CVPR or ECCV\n",
      "AI Reverie Webinar 26May2021\n",
      "Why did you focus on synthetic data?\n",
      "Supervised learning is not scalable .. bla bla. Labelling, annotations. 100,000 images would take months or years to do instance segmentation.\n",
      "What does it take to make synthetic data work in the real-world. How do you answer to the skeptics? - he didn't.\n",
      "---\n",
      "Score: 0.30115212341356884\n",
      "Randomization process:  \"Domain randomization for object detection. Synthetic objects (in this case cars, top-center) are rendered on top of a random\n",
      "background (left) along with random flying distractors (geometric shapes next to the background images) in a scene with random lighting\n",
      "from random viewpoints. Before rendering, random texture is applied to the objects of interest as well as to the flying distractors. The\n",
      "resulting images, along with automatically-generated ground truth (right), are used for training a deep neural network\". for use it could be just background lighting (no viewpoint we only have 1)\n",
      "---\n",
      "Score: 0.2989584424480918\n",
      "Complexity of the label - the flatter the label the quicker and more accurate it will be.\n",
      "Plastic wrapping/ transparent containers - this adds complexity and time.\n",
      "Organic/natural shapes - the closer to spherical (apples) the quicker it will be.\n",
      "Organic/natural shapes - more complex shapes (cauliflower) will require scanning.\n",
      "This  spreadsheet  has examples.\n",
      "Challenges\n",
      "Simple Shapes\n",
      "A larger proportion of CPGs are relatively simple, their shape is orthogonal and symmetrical.\n",
      "---\n",
      "Score: 0.29600417107395205\n",
      "Smart algorithm to create the Dropout for more than 1 product (i.e. can dropout 6 from a type of product) for the change detection, including shifting products nearby.\n",
      "They measure IoU as model performance, and achieve 36%.\n",
      "Our dataset provides multiple views per scene, enabling multi-view representation learning  - I thought it was a nice formulation.\n",
      "2.2  Finally, domain adaptation from synth-to-real is becoming increasingly important [29], since many real world applications require accurate depth maps with no easy way to measure ground truth.  → do you understand this part?\n",
      "Ideas\n",
      "---\n",
      "Score: 0.2954261565353114\n",
      "Results in [4] suggest  congruent clutter  does really well on F&F idealized, suggesting scales are important and backgrounds can be artificially created in such a way to make the network learns the objects of interest. Even though CT could work better on datasets where objects have multiple poses, on our test set it does pretty well. However, the opposite seems to be true for the tests done on F&F generalized. See pending questions.\n",
      "OSA:\n",
      "---\n",
      "Score: 0.2948727246396681\n",
      "This paper introduces domain randomization for object localization. They demonstrate the capabilities by performing grasping in a cluttered environment. Reference paper for introducing the technique.\n",
      "Domain specific\n",
      "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding  2020 ( pdf )\n",
      "Create a synthetic data set from scratch using scenes. They gathered the scenes online. They're added value is creating these rendered data (they output a lot). Uses VRay. Expensive.\n",
      "Github page  here\n",
      "Scene graph\n",
      "Sim2SG: Sim-to-Real Scene Graph Generation for Transfer Learning  2020 ( arxiv )\n",
      "---\n",
      "Score: 0.2923803993292057\n",
      "Question I have: I didn't quite understand how they generate this contextual model, would have to re-read.\n",
      "Random\n",
      "Learning to Resize Images for Computer Vision Tasks  2021 ( arxiv )\n",
      "Resizing images has been given little attention. They come up with a learned image resizer that improves accuracy. Takeaway: not all image sizes are the same.\n",
      "Calin's repo\n",
      "Synthetic Data Generation:\n",
      "AutoSimulate: (Quickly) Learning Synthetic\n",
      "Data Generation\n",
      "propose a differentiable approximation function in order to update the rendering parameters\n",
      "---\n",
      "Score: 0.28898870498238605\n",
      "During the process of taking pictures of the products for the 3D asset creation, if the product is cylindrical or similar to a bottle in shape then the label could be taken off the product, flattened out and a picture could be taken of the flat label or be scanned. This way could minimise the distortion and uneven stitching of the label when its applied to the UV map of the 3D asset. For products that have a box shape or similar (flat faces) then a picture taken as parallel as possible to the face to be digitised will facilitate the process of later on putting the label on the 3D asset. The less perspective and angle that can be achieved between the camera and the product the better the final result will be.\n",
      "---\n",
      "Score: 0.28804005798939547\n",
      "Occlusion layer: determine FG object bbox, place distractor within [10,30] % of that bbox. Pose and color of occluder is randomized.\n",
      "Better than real data, curriculum training much better than random poses, purely synthetic background better than with real. This:\n",
      "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization  2018 ( pdf )\n",
      "Using only non-artistically-generated synthetic data and domain randomization, generate an accurate model. With some additional fine-tuning on real-data, network was better than real only.\n",
      "---\n",
      "Score: 0.2871682697930503\n",
      "To validate if the calculation of the uniqueness make sense, we again can you use a toy example. The toy-dataset is the uvesco image set to which we add an image of an animal and from which we copy one image. We got the following:\n",
      "The most \"unique\" image is the animal image.\n",
      "The less \"unique\" image is the duplicated uvesco image.\n",
      "Validation on uvesco dataset:\n",
      "Next, we do the same for most and less \"unique\" images but applied to uvesco dataset. We get the following:\n",
      "Most unique image\n",
      "Less unique image\n",
      "---\n",
      "Score: 0.2856231365027192\n",
      "Their definition of domain randomization intentionally abandons photorealism, which they say takes too long to produce and thus negates the advantages provided by synthetic data.\n",
      "The parameters that they apply the randomization to are:\n",
      "Number of objects\n",
      "Lighting\n",
      "Viewpoints\n",
      "Texture (Flickr 8K images)\n",
      "Background\n",
      "Addition of flying distractors (number, types, colors, scales randomized)\n",
      "Data augmentations used: brightness, contrast, Gaussian noise.\n",
      "Randomization: fixed lights, texture, data augmentation, flying distractors. Removing them made the accuracy suffer.\n",
      "---\n",
      "Score: 0.28363630142288304\n",
      "To circumvent this problem, synthetic data generation has surfaced as a promising solution. Two main approaches have surfaced, using Generative Adversarial Networks \\cite{Goodfellow2020GenerativeNetworks} to generate synthetic images \\cite{Han2018GAN-basedGeneration, Osokin2017GANsSynthesis, Vega-Marquez2020CreationNetworks} or using a graphics 3D engine to automatically generate the images from a 3D virtual scene \\cite{Dosovitskiy2017CARLA:Simulator, Wu2018BuildingEnvironment, Kolve2017AI2-THOR:AI}. For this project we focus on the latter approach where an arbitrary amount of photos of a controlled 3D scene can be automatically generated for a given task, using a graphical 3D rendering software (e.g. Blender, Unity, etc.). Moreover, since the software allows for a controlled 3D scene, the location of each entity is known, and therefore the annotation of each image can be automatically generated as each image is output. This synthetic data is then used to train an object detection, classification or semantic segmentation model that can be deployed on a real-world task. This approach has shown early success in autonomous driving \\cite{Gaidon2016VirtualWorldsAnalysis, Ros2016TheScenes}, robotics applications \\cite{Todorov2012MuJoCo:Control, Kolve2017AI2-THOR:AI, Puig2018VirtualHome:Programs}, and others \\cite{Shah2017AerialResearch, Butler2012AEvaluation, Brockman2016OpenAIGym}.\n",
      "---\n",
      "Score: 0.28025709987598213\n",
      "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization\n",
      "\"With additional fine-tuning on\n",
      "real data, the network yields better performance than using\n",
      "real data alone\" → perfect fit for scenarios where real data is scarce\n",
      "---\n",
      "Score: 0.27605037884746864\n",
      "In order to create a more realistic product with the specular lights correct takes significantly longer (2 hours).\n",
      "The plastic wrapping requires testing in each renderer.\n",
      "The effects of plastic wrapping require CV testing.\n",
      "Organic Geometry\n",
      "Fruit, vegetables and some breads are very complex shapes and we would advise that SDG get a heads up before taking on these products.\n",
      "---\n",
      "Score: 0.2745721155146092\n",
      "Novel method for rendering with pure synthetic data: background of similar objects, then the objects, then basic geometric occluders. They outperform real data. All of the randomized stats are controlled such that every \"part\" is shown equally to the training.\n",
      "Background scene: 15k separate objects, scaled to unit sphere, projected onto open space with scaling [0.9,1.5] and HSV randomly changed. Camera settings slightly randomized.\n",
      "Foreground layer: generate a large set of poses (curriculum) and scales, deterministically sampled. Random light position, random light color, random noise, random blur.\n",
      "---\n",
      "Score: 0.2745519068868535\n",
      "\"We note that pre-processing the data using super-resolution techniques or gross oversampling (4x or greater) may help to improve performance, but will unfortunately slow training and inference time.\" -  who cares about slow training if you can simply oversample or use super-resolution? What about inference, what does that have to do with anything, the model gets more complex?\n",
      "Conclusions on real data\n",
      "This blog laid out all of the baseline experiments we ran on the real dataset. We observe that:\n",
      "Performance declines as task difficulty becomes more challenging.\n",
      "Even with prior optimization, detectors still struggle with small objects.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_text = \"what factors should you consider when producing a digital replica?\"\n",
    "embedding = model.encode(query_text).tolist()\n",
    "\n",
    "scores = cosine_similarity([embedding], vectors[0])[0] \n",
    "top_scores_ids = np.argsort(scores)[-30:][::-1]\n",
    "\n",
    "# print top 10 results\n",
    "for i in top_scores_ids:\n",
    "    print(f\"Score: {scores[i]}\")\n",
    "    for meta_ in meta[i]:\n",
    "        print(meta_[\"text\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(unnested_meta).to_csv(\n",
    "    DATA / \"processed\" / \"notion_research_9462314059bd4a9e8a997d481bf6b009.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7dcbf1e03c02e24dc4a584a70e4e23a95f5bc1447200fb3184f76f97c0e1aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
